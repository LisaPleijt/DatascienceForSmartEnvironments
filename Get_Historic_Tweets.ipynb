{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook all data concerning the harvesting of tweets is present.\n",
    "The output of this notebook is a csv containing the necessary data for the analyses in the later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import tweepy\n",
    "import os\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import dataset #https://dataset.readthedocs.io/en/latest/api.html\n",
    "from datafreeze import freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection with the API, add keys \n",
    "\n",
    "# Key\n",
    "Key1 = \"\"\n",
    "\n",
    "# Secret Key\n",
    "Key2 = \"\" \n",
    "\n",
    "# Generate the access tokens in developer account: https://developer.twitter.com/en/portal/dashboard\n",
    "# See screenshot below\n",
    "\n",
    "# Access token\n",
    "ACCESS_TOKEN = \"\"\n",
    "\n",
    "# Access token secret\n",
    "ACCESS_TOKEN_SECRET = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# Set up and test API\n",
    "\n",
    "# Authorize \n",
    "auth = tweepy.OAuthHandler(Key1, Key2)\n",
    "\n",
    "# Authorize with access token\n",
    "auth.set_access_token(ACCESS_TOKEN,ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "          wait_on_rate_limit_notify=True)\n",
    "\n",
    "# Test authorization\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Global variables for harvesting of tweets\n",
    "\n",
    "# Database Variables\n",
    "CONNECTION_STRING = \"sqlite:///tweets_historic_2.db\"\n",
    "CSV_NAME = \"tweets_historic_full_text.csv\"\n",
    "TABLE_NAME = \"tweets_historic\"\n",
    "\n",
    "# API Rate Limit Variables\n",
    "MAX_REQUEST = 1000\n",
    "PAUSE_TIME = 960 # 16 minutes\n",
    "\n",
    "# Open Database\n",
    "db2 = dataset.connect(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''clean text of links, special characters, @mentions, \n",
    "    double spaces, lowercase everthing'''\n",
    "    cleaned = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    cleaned = cleaned.lower()\n",
    "    return(cleaned)\n",
    "\n",
    "def create_db_historic_tweets(number, NTweets, last_max_id):\n",
    "    '''request tweets from API and add them to the \n",
    "    DB in the right format'''\n",
    "    \n",
    "    #get tweets with query\n",
    "    statuses = tweepy.Cursor(api.search, q='(covid OR corona) -filter:retweets', \n",
    "                             tweet_mode = 'extended', lang = \"nl\", max_id = last_max_id).items(number)\n",
    "    \n",
    "    #save max_id to be able to get increasingly old tweets each cycle\n",
    "    max_status_id = 0\n",
    "    \n",
    "    #main loop for saving statusses to DB \n",
    "    for status in statuses:\n",
    "        description = status.user.description\n",
    "        loc = status.user.location\n",
    "        coords = status.coordinates\n",
    "        geo = status.geo\n",
    "        name = status.user.screen_name\n",
    "        user_created = status.user.created_at\n",
    "        followers = status.user.followers_count\n",
    "        id_str = status.id_str\n",
    "        created = status.created_at\n",
    "        retweets = status.retweet_count\n",
    "        bg_color = status.user.profile_background_color\n",
    "        text = status.full_text\n",
    "\n",
    "        if geo is not None:\n",
    "            geo = json.dumps(geo)\n",
    "\n",
    "        if coords is not None:\n",
    "            coords = json.dumps(coords)   \n",
    "\n",
    "        table = db2[TABLE_NAME]\n",
    "        max_status_id = id_str  \n",
    "\n",
    "        text = clean_text(text)\n",
    "        loc = clean_text(loc)\n",
    "        try: \n",
    "            table.insert(dict(\n",
    "                user_description=description,\n",
    "                user_location=loc,\n",
    "                coordinates=coords,\n",
    "                text=text,\n",
    "                geo=geo,\n",
    "                user_name=name,\n",
    "                user_created=user_created,\n",
    "                user_followers=followers,\n",
    "                id_str=id_str,\n",
    "                created=created,\n",
    "                retweet_count=retweets,\n",
    "                user_bg_color=bg_color,\n",
    "            ))\n",
    "        except:\n",
    "            print(\"error\")\n",
    "    print(NTweets + MAX_REQUEST, \"tweets at Time: %s\" % time.ctime())\n",
    "    return NTweets + MAX_REQUEST, max_status_id\n",
    "\n",
    "\n",
    "def night_runner():\n",
    "    '''Main loop for requesting tweets, pauses after each request'''\n",
    "    # reset below variables with last tweet-id each time the API disconnects\n",
    "    NTweets = 57000  \n",
    "    last_max_id = \"1382689495179485191\"\n",
    "    while(True):\n",
    "        NTweets, last_max_id = create_db_historic_tweets(MAX_REQUEST, NTweets, last_max_id)\n",
    "        time.sleep(PAUSE_TIME)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run main loop to collect tweets in DB \n",
    "night_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database \n",
    "db2 = dataset.connect(\"sqlite:///tweets_historic.db\")\n",
    "\n",
    "# Store the resulting DB in a csv\n",
    "result = db2[TABLE_NAME].all()\n",
    "\n",
    "# Export result to csv \n",
    "freeze(result, format='csv', filename=CSV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is some extra code to test the process and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv and print length \n",
    "data = pd.read_csv(CSV_NAME)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display data \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### (Design)Choices\n",
    "Tweepy: Because of the extensive and easy (or easier) to understand documentation, compared to for example twython, we decided to use Tweepy to handle the connection with the API \n",
    "\n",
    "Database: We decided to add the tweets directly to a DB and output the DB to a csv afterwards. This way we could add tweets to the database one-by-one, and if the API was interupted we could easily restart it without causing problems with the already harvested data.\n",
    "\n",
    "\n",
    "#### Problems\n",
    "API Documentation was very complex, and often outated because of the recently implemented V2 API of twitter. It took a long time to get the API running. \n",
    "\n",
    "On the first run we collected 15.000 tweets, but found out afterwards that we did not collect the full text tweets. This happened because we only tested the query for full text in the Live API, and assumed it would also work in the REST API. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
